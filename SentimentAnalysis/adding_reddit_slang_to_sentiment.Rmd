---
title: "adding_reddit_slang_to_sentiment"
author: "Julius Antonio Bladt"
date: "23/03/2021"
output: html_document
---

```{R}
library(readr)
gme_comments <- read_delim("C:/Users/Julius Antonio Bladt/Desktop/GMEComments.csv", delim = ";")
View(gme_comments)

#gme_sample <- gme_comments[sample(nrow(gme_comments), 100), ]
#view(gme_sample)

library(tidyverse)
library(tidytext)
library(ggplot2)

reddit_words <- gme_comments %>% select(id, body) %>% unnest_tokens(word,body) # unnest_tokens will split a column into tokens, flattening the table into one-token-per-row

my_stop_words <- stop_words %>% select(-lexicon) # Creating object stop words

reddit_words_interesting <- reddit_words %>% anti_join(my_stop_words)


# RUN THIS PLOT IN CONSOLE IF IT DOES NOT WORK
reddit_words_interesting  %>% group_by(word) %>% tally(sort=TRUE) %>% slice(1:25) %>% 
    ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1)) + xlab("")


loughran <- get_sentiments("loughran") 
grep("moon", loughran) # Checking whether the given word is in the lexicon

```



Words not captured by the lexicon we are using are "apes" and "moon". "apes" refers to individuals in the subreddit Wallstreetbets calling each other apes, given many of them discusses not conducting proper due dilligence in relation to their investments. This does not necesarily help us in predicting stock prices, since it does not refer to anything related to actually buying/selling individual stocks. 

Now I add "moon" to the stop word list and view whether new words pop up, that should be added to the lexicon:

```{R}

my_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("moon", "gme", "buy", "shares", "amc", "stock", "time", "day", "hold","money","https","price","sell","market","bought","people","apes","tommorow","short","fuck")))# Adding moon to the stop words, and removing all other most popular words to evaluate new words for the lexicon list

reddit_words_interesting <- reddit_words %>% anti_join(my_stop_words)

reddit_words_interesting  %>% group_by(word) %>% tally(sort=TRUE) %>% slice(1:25) %>% 
    ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1)) + xlab("")

grep("dip", loughran) # Checking whether the given word is in the lexicon
grep("squeeze", loughran)

```

"Dip" is another word frequently used on Reddit, which means that a stock is going to "dip", i.e. drop in price. This will definitely need to be added to a negative category in the McDonald & Loghran lexicon. Another interesting word is "stimmy", which refers to the stimulus check of $600 paid to Americans. While this word implies individuals having more capital to invest, it does not indicate anything about whether they will actually invest or not (i.e. "the stimmy is going to help cover my loss and buy food"). Thus, we will not add it to the lexicon. The word "squeeze" refers to Wallstreetbets trying to achieve a short squeeze with the GME stock, similar to that of VW in 2008 (see report). In the short term, this means that the price will roar up, given that investors with short positions will have to cover their losses. And since we want to predict stock movements in the short term, we definitely need to add "squeeze" to a positive category in the lexicon.

```{R}
my_stop_words <- stop_words %>% select(-lexicon) %>% 
  bind_rows(data.frame(word = c("moon", "gme", "buy", "shares", "amc", "stock", "time", "day", "hold","money","https","price","sell","market","bought","people","apes","tommorow","short","fuck", "fucking", "tommorow", "buying", "dip", "shit", "holding", "week", "sold", "share", "gonna", "squeeze", "stocks","https", "hits", "10", "300", "2","stimmy"))# Adding moon to the stop words, and removing all other most popular words to evaluate new words for the lexicon list

reddit_words_interesting <- reddit_words %>% anti_join(my_stop_words)

reddit_words_interesting  %>% group_by(word) %>% tally(sort=TRUE) %>% slice(1:25) %>% 
    ggplot(aes(x = reorder(word, n, function(n) -n), y = n)) + geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1)) + xlab("")

```



```{R}

custom_lexicon <- loughran %>%
  bind_rows(tribble(~word, ~sentiment,
                    "moon", "positive",
                    "dip", "negtaive",
                    "squeeze", "positive"))


```

############################################################################################################################

Now, we have taken care of the most popular words. 

